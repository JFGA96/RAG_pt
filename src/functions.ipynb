{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgarzoad.EPMCC-POB\\.conda\\envs\\demoRAG\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\jgarzoad.EPMCC-POB\\.conda\\envs\\demoRAG\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureOpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from qdrant_client import QdrantClient\n",
    "import numpy as np\n",
    "from langgraph.graph import END, StateGraph\n",
    "from prompt import route_chain,rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_llm import llm_embed_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://0ff2fdbb-b73d-4afb-a722-b0ccf5a75cc5.eu-west-2-0.aws.cloud.qdrant.io\", \n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.-Sum4wHCItZwbIel2L3oN8GMkfihHwRqPhAvM_QeXOk\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_by_index(vectorstore, target_index: int) -> Document:\n",
    "    \"\"\"\n",
    "    Retrieve a chunk from the vectorstore based on its index in the metadata.\n",
    "    \n",
    "    Args:\n",
    "    vectorstore (VectorStore): The vectorstore containing the chunks.\n",
    "    target_index (int): The index of the chunk to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    Optional[Document]: The retrieved chunk as a Document object, or None if not found.\n",
    "    \"\"\"\n",
    "    # This is a simplified version. In practice, you might need a more efficient method\n",
    "    # to retrieve chunks by index, depending on your vectorstore implementation.\n",
    "    query_vector = np.zeros(1536).tolist()\n",
    "\n",
    "    all_docs = vectorstore.search(\n",
    "    collection_name=\"model_v2\",\n",
    "    query_vector=query_vector,\n",
    "    limit=vectorstore.get_collection(\"model_v2\").points_count  \n",
    "    )\n",
    "\n",
    "    for doc in all_docs:\n",
    "        if doc.payload.get(\"index\") == target_index:\n",
    "            return doc\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_retrieve_with_context(index,vectorstore, num_neighbors: int) -> str:\n",
    "\n",
    "    current_index = index\n",
    "    start_index = max(1, current_index - num_neighbors)\n",
    "    end_index = current_index + num_neighbors + 1\n",
    "\n",
    "    # Retrieve all chunks in the range\n",
    "    neighbor_chunks = []\n",
    "    for i in range(start_index, end_index):\n",
    "        try:\n",
    "            neighbor_chunk = get_chunk_by_index(vectorstore, i)\n",
    "            neighbor_chunks.append(neighbor_chunk)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    text_chunks = []\n",
    "    for chunk in neighbor_chunks:\n",
    "        text_chunks.append(chunk.payload[\"text\"])\n",
    "\n",
    "    text_to_retrieve = \" \".join(text_chunks)\n",
    "\n",
    "    return text_to_retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents \n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    documents : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to otros,process or RAG \n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = route_chain.invoke({\"question\": question})\n",
    "    try:\n",
    "        result = source['answer']\n",
    "        return result\n",
    "    except:\n",
    "        result = \"otros\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE from Vector Store DB---\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "\n",
    "    chunks_query_retriever = qdrant_client.search(\n",
    "    collection_name=\"model_v2\",\n",
    "    query_vector=llm_embed_small.embed_query(question),\n",
    "    limit=1\n",
    "    )\n",
    "\n",
    "    doc_index = chunks_query_retriever[0].payload['index']\n",
    "\n",
    "    filtered_docs = index_retrieve_with_context(\n",
    "        index = doc_index,\n",
    "        vectorstore = qdrant_client,\n",
    "        num_neighbors = 1\n",
    "        )\n",
    "    \n",
    "\n",
    "    print(filtered_docs)\n",
    "    return {\"documents\": filtered_docs, \"question\": question,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE Answer---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    print(generation)\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers_predefined(state):\n",
    "    question = state['question']\n",
    "    if \"documents\" in state:\n",
    "        documents = state['documents']\n",
    "        if documents == None:\n",
    "            return {'question':question,'generation':\"Actualmente solo puedo con los datos mencionados en el documento de prueba\",\"documents\":\"\"}\n",
    "        else:\n",
    "            return {'question':question,'generation':\"En el momento no cuento con el contexto necesario para responder tu pregunta\",\"documents\":\"\"}\n",
    "    else:\n",
    "        return {'question':question,'generation':\"Actualmente solo puedo ayudarte con tu consulta\",\"documents\":\"\"}\n",
    "    \n",
    "def initial(state):\n",
    "    return \"route_question\"\n",
    "\n",
    "def retrieve_to_generate(state):\n",
    "    return \"generate\"\n",
    "\n",
    "def retrieve_process_to_generate(state):\n",
    "    return \"generate_process\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"predefined\", answers_predefined) # answers predefined\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"generate\", generate) # generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"data\": \"retrieve\",\n",
    "        \"otros\": \"predefined\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\", # start: node\n",
    "    retrieve_to_generate, # defined function\n",
    "    {\n",
    "        \"generate\": \"generate\", #returns of the function\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_langgraph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flujo(input):\n",
    "    for output in app_langgraph.stream(input):\n",
    "        for key, value in output.items():\n",
    "            print(f\"Finished running: {key}:\")\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---RETRIEVE from Vector Store DB---\n",
      "in-house capabilities that will help carriers secure the needed support for business evolution and execution. The IT architecture of the future will also be radically different from today's. Carriers should start making targeted investments to enable the migration to a more future-forward technology stack that can support a two-speed IT architecture. Rapid advances in technologies in the next decade will lead to disruptive changes in the insurance industry. The winners in AI-based insurance will be carriers that use new technologies to create innovative products, harness cognitive learning insights from new data sources, streamline processes and lower costs, and exceed customer expectations for individualization and dynamic adaptation. Most important, carriers\n",
      "\n",
      "that adopt a mind-set focused on creating opportunities from disruptive technologies -instead of viewing them as a threat to their current business-will thrive in the insurance industry in 2030. - 1. Convolutional neural networks contain millions of simulated 'neurons' structured in layers.\n",
      "- 2. Deep shift: Technology tipping points and societal impact, World Economic Forum, September 2015, weforum.org.\n",
      "- 3. Some insurtech companies are already beginning to design these types of products; Slice, for example, provides variable commercial insurance specifically tailored for home sharing.\n",
      "- 4. This shift to a more automated claims function has already begun. Fukoku Mutual Life Insurance, for example, has been using IBM's Watson Explorer since January 2017 to do the work of 34 claims adjusters-30 percent of its claims staff.\n",
      "Finished running: retrieve:\n",
      "---GENERATE Answer---\n",
      "Las aseguradoras que adopten inteligencia artificial (IA) tendrán una ventaja competitiva significativa debido a su capacidad para innovar y adaptarse a un entorno en constante cambio. En primer lugar, la implementación de tecnologías avanzadas permitirá a las aseguradoras crear productos innovadores y personalizados, lo que responde a las crecientes expectativas de los clientes en cuanto a individualización y adaptación dinámica.\n",
      "\n",
      "Además, el uso de IA y el análisis de datos permitirá a las aseguradoras aprovechar conocimientos cognitivos que mejorarán la toma de decisiones y la eficiencia operativa. Por ejemplo, la automatización de funciones como el procesamiento de reclamaciones, tal como lo demuestra el caso de Fukoku Mutual Life Insurance, que utiliza IBM Watson Explorer, puede reducir los costos y mejorar la rapidez en el servicio al cliente.\n",
      "\n",
      "Finalmente, las aseguradoras que adopten un enfoque proactivo, viendo las tecnologías disruptivas como oportunidades en lugar de amenazas, estarán mejor posicionadas para prosperar en el futuro. Al realizar inversiones estratégicas en una arquitectura tecnológica más avanzada, estas empresas no solo mejorarán su capacidad operativa, sino que también estarán preparadas para enfrentar los retos de la industria en 2030.\n",
      "Finished running: generate:\n",
      "Las aseguradoras que adopten inteligencia artificial (IA) tendrán una ventaja competitiva significativa debido a su capacidad para innovar y adaptarse a un entorno en constante cambio. En primer lugar, la implementación de tecnologías avanzadas permitirá a las aseguradoras crear productos innovadores y personalizados, lo que responde a las crecientes expectativas de los clientes en cuanto a individualización y adaptación dinámica.\n",
      "\n",
      "Además, el uso de IA y el análisis de datos permitirá a las aseguradoras aprovechar conocimientos cognitivos que mejorarán la toma de decisiones y la eficiencia operativa. Por ejemplo, la automatización de funciones como el procesamiento de reclamaciones, tal como lo demuestra el caso de Fukoku Mutual Life Insurance, que utiliza IBM Watson Explorer, puede reducir los costos y mejorar la rapidez en el servicio al cliente.\n",
      "\n",
      "Finalmente, las aseguradoras que adopten un enfoque proactivo, viendo las tecnologías disruptivas como oportunidades en lugar de amenazas, estarán mejor posicionadas para prosperar en el futuro. Al realizar inversiones estratégicas en una arquitectura tecnológica más avanzada, estas empresas no solo mejorarán su capacidad operativa, sino que también estarán preparadas para enfrentar los retos de la industria en 2030.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"¿Por qué las aseguradoras que adopten IA tendrán ventaja competitiva?\"}\n",
    "for output in app_langgraph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Finished running: {key}:\")\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demoRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
